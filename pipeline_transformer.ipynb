{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 构造word_embedding\n",
    "\n",
    "### 构造position embeddings\n",
    "$$ PE(pos, 2i) = sin(pos/10000^{2i/d_{model}})  $$\n",
    "$$ PE(pos, 2i+1) = cos(pos/1000^{2i/d_{model}})  $$\n",
    "\n",
    " where $pos$ is the position and $i$ is the dimension. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 2])\n",
      "tensor([4, 2])\n",
      "tensor([[4, 1, 0, 0, 0],\n",
      "        [2, 6, 0, 0, 0]])\n",
      "tensor([[0, 1],\n",
      "        [0, 1]], dtype=torch.int32)\n",
      "tensor([[0, 1, 2, 3],\n",
      "        [0, 1, 2, 3]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# 关于word embedding，以序列建模为例\n",
    "# 考虑source sentence 和 target sentence\n",
    "# 构建序列，序列的词符以其在词表中的索引形式表示\n",
    "\n",
    "batch_size = 2\n",
    "\n",
    "# 单词表大小\n",
    "max_num_src_words = 8\n",
    "max_num_tgt_words = 8\n",
    "model_dim = 8 # 原始论文512\n",
    "\n",
    "# 序列的最大长度\n",
    "max_src_seq_len = 5\n",
    "max_tgt_seq_len = 5\n",
    "max_position_len = 5\n",
    "\n",
    "# src_len 和 tgt_len 是每个批次的源和目标序列长度\n",
    "src_len = torch.randint(2, 5, (batch_size,))\n",
    "tgt_len = torch.randint(2, 5, (batch_size,))\n",
    "\n",
    "# 单词索引构成源句子和目标句子，并且做了padding，默认值为0\n",
    "src_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_src_words, (L,)), (0, max_src_seq_len-L)), 0) for L in src_len])\n",
    "tgt_seq = torch.cat([torch.unsqueeze(F.pad(torch.randint(1, max_num_tgt_words, (L,)), (0, max_tgt_seq_len-L)), 0) for L in tgt_len])\n",
    "\n",
    "# 构造word embedding\n",
    "## 构建embedding table\n",
    "src_embedding_table = nn.Embedding(max_num_src_words + 1, model_dim) # 当加 +1 时，通常是为了为 填充符号（padding token） 留出一个额外的位置：\n",
    "tgt_embedding_table = nn.Embedding(max_num_tgt_words + 1, model_dim)\n",
    "## 依据table构建embedding\n",
    "src_embdding = src_embedding_table(src_seq)\n",
    "tgt_embdding = tgt_embedding_table(tgt_seq)\n",
    "\n",
    "# position embedding 注意pos代表行，i代表列，分别反映位置变化和维度变化\n",
    "pos_mat = torch.arange(max_position_len).reshape((-1, 1))  \n",
    "i_mat = torch.pow(10000, torch.arange(0, model_dim, 2).reshape(([1, -1])) / model_dim)\n",
    "pe_embedding_table = torch.zeros(max_position_len, model_dim)\n",
    "\n",
    "pe_embedding_table[:, 0::2] = torch.sin(pos_mat / i_mat)  # 偶数维度使用sin\n",
    "pe_embedding_table[:, 1::2] = torch.cos(pos_mat / i_mat)  # 奇数维度使用cos\n",
    "\n",
    "pe_embedding = nn.Embedding(max_position_len, model_dim)\n",
    "pe_embedding.weight = nn.Parameter(pe_embedding_table, requires_grad=False)\n",
    "\n",
    "# 生成源序列和目标序列的position embedding\n",
    "src_pos = torch.cat([torch.unsqueeze(torch.arange(max(src_len)), 0) for _ in src_len]).to(torch.int32)\n",
    "tgt_pos = torch.cat([torch.unsqueeze(torch.arange(max(tgt_len)), 0) for _ in tgt_len]).to(torch.int32)\n",
    "src_pe_embedding = pe_embedding(src_pos)\n",
    "tgt_pe_embedding = pe_embedding(tgt_pos)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(src_len)\n",
    "print(tgt_len)\n",
    "print(src_seq)\n",
    "# print(src_embedding_table.weight) \n",
    "# print(src_embdding.size())\n",
    "print(src_pos)\n",
    "print(tgt_pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
